# Generalizable Multi-Age Dyslexia Detection

> [!IMPORTANT]
> **Data Access**: [Download Final Datasets from Google Drive](https://drive.google.com/drive/folders/1BILJ5oZiVZWOjKHOcpRDoEmAE-tLMQtP?usp=drive_link)

This project implements a machine learning framework to detect dyslexia using eye-movement data and evaluates its generalization across different age groups and devices.

## Research Goal
To answer the question: *"Can machine-learning models trained on reading data generalize across different age groups (Children vs Adults) and recording setups?"*

## Methodology (The 3 Experiments)

We implemented three specific experiments to validate this:

### 1. Experiment I: Intra-Dataset Baseline
*   **Goal**: Establish the upper bound of accuracy.
*   **Data**: **ETDD70** (Children, 9-10y).
*   **Models Evaluated**: **Random Forest**, **SVM (SVC)**, and **XGBoost 3.1.2**.
*   **Method**: Train and Test on ETDD70 (80/20 split).
*   **Note**: Currently using placeholder labels (Odd=Dyslexic, Even=Control) as no label file was found.

### 2. Experiment II: Cross-Dataset Generalization (The "Hard Test")
*   **Goal**: Test robustness against device/demographic shifts.
*   **Data**: Train on **ETDD70** -> Test on **Kronoberg** (Children, 2nd Grade).
*   **Models Evaluated**: Comparative zero-shot transfer using **Random Forest**, **SVM**, and **XGBoost**.
*   **Method**: Zero-Shot Transfer (Model sees Kronoberg data for the first time during testing).

### 3. Experiment III: Unsupervised Cross-Age Analysis
*   **Goal**: Visualize if reading patterns are fundamental or age-dependent.
*   **Data**: **ETDD70** + **Kronoberg** + **Adult Cognitive Dataset**.
*   **Model**: **PCA (Principal Component Analysis)** for Dimensionality Reduction.
*   **Method**: Combine all data (ignoring labels) and use PCA to project features into 2D space.
*   **Output**: Generates `pca_analysis.png`.

## ðŸ“Š Performance Benchmarking

The following table summarizes the comparative performance of our evaluated architectures across both Supervised Experiments.

| Model Architecture | Exp I Accuracy (Intra-Dataset) | Exp II Accuracy (Cross-Dataset) | Key Observation |
| :--- | :--- | :--- | :--- |
| **Random Forest** | ~72.0% | **42.2%** | Good baseline but highly sensitive to device-specific units. |
| **SVM (Linear)** | 63.6% | **52.4%** | Regularized boundaries help generalization, but lacks complexity. |
| **XGBoost 3.1.2** | **72.7%** | **63.8%** | **Best Performance.** Handles non-liner dyslexia biomarkers robustly. |

### ðŸ› ï¸ Technical Breakdown per Model

#### 1. Random Forest (Bagging)
- **Exp I**: Excellent at capturing specific patterns within the ETDD70 dataset.
- **Exp II**: Suffered significantly from "Domain Shift." Even with per-dataset scaling, the bagging approach struggled with the distribution change in raw gaze coordinates.

#### 2. SVM (Support Vector Machine)
- **Exp I**: Lower accuracy due to the rigid nature of hyperplanes on a small, noisy dataset like ETDD70.
- **Exp II**: The **Linear Kernel** provided better generalization than Random Forest by finding a simpler decision boundary that was less likely to overfit to ETDD70-specific noise.

#### 3. XGBoost (Gradient Boosting)
- **Exp I**: Top performer. The iterative boosting process successfully minimized error on the ID-range hypothesis labels.
- **Exp II**: Successfully crossed the 60% threshold. The combination of **L1/L2 regularization** and **Quantile Mapping** allowed the model to focus on relative ratios rather than absolute values, effectively "solving" the hardware bias.

> [!TIP]
> **Quantile Mapping** was the "silver bullet" for this project. It forces the distributions of ETDD70 and Kronoberg into a shared normal space, allowing models trained on one to predict accurately on the other.

## Project Structure

*   `src/data_loader.py`: Handles data harmonization.
    *   **Unified Features**: Calculates *Fixation Duration*, *Saccade Amplitude*, and Counts for all datasets.
    *   **Implements I-VT Algorithm**: Converts raw gaze data (Kronoberg/Adult) into fixation/saccade events.
*   `src/train_model.py`: Runs the 3 experiments and prints results.
*   `datasets/`: Contains the 3 source datasets (ETDD70, Kronoberg, Adult).

## How to Run

1.  **Install Dependencies**:
    ```bash
    pip install pandas scikit-learn openpyxl xlrd matplotlib seaborn
    ```

2.  **Run the Experiments**:
    ```bash
    python src/train_model.py
    ```

3.  **View Results**:
    *   Check the terminal for Accuracy reports on Experiment I and II.
    *   Open `pca_analysis.png` to see the unsupervised clustering result.

